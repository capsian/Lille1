{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN Presentation\n",
    "------------------------------------------------------\n",
    "# Authors\n",
    "    - Selim Lakhdar\n",
    "        - selim.lakhdar.etu@univ-lille.fr\n",
    "    - Josue Happe\n",
    "        - josue.hape.etu@univ-lille.fr\n",
    "------------------------------------------------------\n",
    "\n",
    "# Part I\n",
    "- Le premier exemple est inspiré de Graph Random Neural Network(GRAND) que propose la librairie DGL.\n",
    "    - https://github.com/dmlc/dgl/tree/master/examples/pytorch/grand\n",
    "    - https://arxiv.org/abs/2005.11079\n",
    "    \n",
    "### Abstract\n",
    "We study the problem of semi-supervised learning on graphs, for which graph neural networks (GNNs) have been extensively explored. However, most existing GNNs inherently suffer from the limitations of over-smoothing, non-robustness, and weak-generalization when labeled nodes are scarce. In this paper, we propose a simple yet effective framework -- GRAPH RANDOM NEURAL NETWORKS (GRAND) -- to address these issues. In GRAND, we first design a random propagation strategy to perform graph data augmentation. Then we leverage consistency regularization to optimize the prediction consistency of unlabeled nodes across different data augmentations. Extensive experiments on graph benchmark datasets suggest that GRAND significantly outperforms state-of-the-art GNN baselines on semi-supervised node classification. Finally, we show that GRAND mitigates the issues of over-smoothing and non-robustness, exhibiting better generalization behavior than existing GNNs. The source code of GRAND is publicly available at https://github.com/THUDM/GRAND \n",
    "\n",
    "----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Part II\n",
    "- Dans la seconde partie nous avons essayé un GraphCONV sur le dataset CoauthorCSDataset. Le but est de prédire la catégorie d'un article en ayant déjà quelques observations. Chaque noeud représente un word count vector comme features.\n",
    "```\n",
    "Coauthor CS and Coauthor Physics are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge. Here, nodes are authors, that are connected by an edge if they co-authored a paper; node features represent paper keywords for each author’s papers, and class labels indicate most active fields of study for each author.\n",
    "```\n",
    "    - Statistics:\n",
    "        - Nodes: 18,333\n",
    "        - Edges: 327,576\n",
    "        - Number of classes: 15\n",
    "        - Node feature size: 6,805\n",
    "----------------------------------------------------------\n",
    "\n",
    "\n",
    "# Part III\n",
    "- Dans la 3e partie nous avons fait de la prédiction de liens sur le dataset CoraGraphDataset.\n",
    "```\n",
    "Cora citation network dataset : Nodes mean paper and edges mean citation relationships. Each node has a predefined feature with 1433 dimensions. The dataset is designed for the node classification task. The task is to predict the category of certain paper.\n",
    "```\n",
    "    - Statistics:\n",
    "        - Nodes: 2708\n",
    "        - Edges: 10556\n",
    "        - Number of Classes: 7\n",
    "        \n",
    "----------------------------------------------------------\n",
    "----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "from dgl.data import CoraGraphDataset, CiteseerGraphDataset, PubmedGraphDataset\n",
    "import dgl.function as fn\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import scipy.sparse as sp\n",
    "\n",
    "if th.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(\"device:\", device)\n",
    "# force it\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgl.__version__==\"0.7.2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "  NumNodes: 3327\n",
      "  NumEdges: 9228\n",
      "  NumFeats: 3703\n",
      "  NumClasses: 6\n",
      "  NumTrainingSamples: 120\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "  NumNodes: 19717\n",
      "  NumEdges: 88651\n",
      "  NumFeats: 500\n",
      "  NumClasses: 3\n",
      "  NumTrainingSamples: 60\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'CoraGraphDataset': CoraGraphDataset(),\n",
    "    'CiteseerGraphDataset': CiteseerGraphDataset(),\n",
    "    'PubmedGraphDataset': PubmedGraphDataset()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I\n",
    "    - https://arxiv.org/pdf/2005.11079.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "To effectively augment graph data, we propose random propagation in GRAND, wherein each node’s\n",
    "features can be randomly dropped either partially (dropout) or entirely, after which the perturbed\n",
    "feature matrix is propagated over the graph. As a result, each node is enabled to be insensitive\n",
    "to specific neighborhoods, increasing the robustness of GRAND. Further, the design of random\n",
    "propagation can naturally separate feature propagation and transformation, which are commonly\n",
    "coupled with each other in most GNNs. This empowers GRAND to safely perform higher-order feature\n",
    "propagation without increasing the complexity, reducing the risk of over-smoothing for GRAND. More\n",
    "importantly, random propagation enables each node to randomly pass messages to its neighborhoods.\n",
    "Under the assumption of homophily of graph data [30], we are able to stochastically generate different\n",
    "augmented representations for each node. We then utilize consistency regularization to enforce the\n",
    "prediction model, e.g., a simple Multilayer Perception (MLP), to output similar predictions on\n",
    "different augmentations of the same unlabeled data, improving GRAND’s generalization behavior\n",
    "under the semi-supervised setting.\n",
    "Finally, we theoretically illustrate that random propagation and consistency regularization can enforce\n",
    "the consistency of classification confidence between each node and its multi-hop neighborhoods.\n",
    "Empirically, we also show both strategies can improve the generalization of GRAND, and mitigate the\n",
    "issues of non-robustness and over-smoothing that are commonly faced by existing GNNs. Altogether,\n",
    "extensive experiments demonstrate that GRAND achieves state-of-the-art semi-supervised learning\n",
    "results on GNN benchmark datasets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](grand_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_node(feats, drop_rate, training):\n",
    "    \n",
    "    n = feats.shape[0]\n",
    "    drop_rates = th.FloatTensor(np.ones(n) * drop_rate)\n",
    "    \n",
    "    if training:\n",
    "            \n",
    "        masks = th.bernoulli(1. - drop_rates).unsqueeze(1)\n",
    "        feats = masks.to(feats.device) * feats\n",
    "        \n",
    "    else:\n",
    "        feats = feats * (1. - drop_rate)\n",
    "\n",
    "    return feats\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, input_droprate, hidden_droprate, use_bn =False):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(nfeat, nhid, bias = True)\n",
    "        self.layer2 = nn.Linear(nhid, nclass, bias = True)\n",
    "\n",
    "        self.input_dropout = nn.Dropout(input_droprate)\n",
    "        self.hidden_dropout = nn.Dropout(hidden_droprate)\n",
    "        self.bn1 = nn.BatchNorm1d(nfeat)\n",
    "        self.bn2 = nn.BatchNorm1d(nhid)\n",
    "        self.use_bn = use_bn\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.layer1.reset_parameters()\n",
    "        self.layer2.reset_parameters()\n",
    "        \n",
    "    def forward(self, x):\n",
    "         \n",
    "        if self.use_bn: \n",
    "            x = self.bn1(x)\n",
    "        x = self.input_dropout(x)\n",
    "        x = F.relu(self.layer1(x))\n",
    "        \n",
    "        if self.use_bn:\n",
    "            x = self.bn2(x)\n",
    "        x = self.hidden_dropout(x)\n",
    "        x = self.layer2(x)\n",
    "\n",
    "        return x   \n",
    "        \n",
    "\n",
    "def GRANDConv(graph, feats, order):\n",
    "    '''\n",
    "    Parameters\n",
    "    -----------\n",
    "    graph: dgl.Graph\n",
    "        The input graph\n",
    "    feats: Tensor (n_nodes * feat_dim)\n",
    "        Node features\n",
    "    order: int \n",
    "        Propagation Steps\n",
    "    '''\n",
    "    with graph.local_scope():\n",
    "        \n",
    "        ''' Calculate Symmetric normalized adjacency matrix   \\hat{A} '''\n",
    "        degs = graph.in_degrees().float().clamp(min=1)\n",
    "        norm = th.pow(degs, -0.5).to(feats.device).unsqueeze(1)\n",
    "\n",
    "        graph.ndata['norm'] = norm\n",
    "        graph.apply_edges(fn.u_mul_v('norm', 'norm', 'weight')) \n",
    "        \n",
    "        ''' Graph Conv '''\n",
    "        x = feats\n",
    "        y = 0+feats\n",
    "\n",
    "        for i in range(order):\n",
    "            graph.ndata['h'] = x\n",
    "            graph.update_all(fn.u_mul_e('h', 'weight', 'm'), fn.sum('m', 'h'))\n",
    "            x = graph.ndata.pop('h')\n",
    "            y.add_(x)\n",
    "\n",
    "    return y /(order + 1)\n",
    "\n",
    "class GRAND(nn.Module):\n",
    "    r\"\"\"\n",
    "    Parameters\n",
    "    -----------\n",
    "    in_dim: int\n",
    "        Input feature size. i.e, the number of dimensions of: math: `H^{(i)}`.\n",
    "    hid_dim: int\n",
    "        Hidden feature size.\n",
    "    n_class: int\n",
    "        Number of classes.\n",
    "    S: int\n",
    "        Number of Augmentation samples\n",
    "    K: int\n",
    "        Number of Propagation Steps\n",
    "    node_dropout: float\n",
    "        Dropout rate on node features.\n",
    "    input_dropout: float\n",
    "        Dropout rate of the input layer of a MLP\n",
    "    hidden_dropout: float\n",
    "        Dropout rate of the hidden layer of a MLPx\n",
    "    batchnorm: bool, optional\n",
    "        If True, use batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 in_dim,\n",
    "                 hid_dim,\n",
    "                 n_class,\n",
    "                 S = 1,\n",
    "                 K = 3,\n",
    "                 node_dropout=0.0,\n",
    "                 input_droprate = 0.0, \n",
    "                 hidden_droprate = 0.0,\n",
    "                 batchnorm=False):\n",
    "\n",
    "        super(GRAND, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.S = S\n",
    "        self.K = K\n",
    "        self.n_class = n_class\n",
    "        \n",
    "        self.mlp = MLP(in_dim, hid_dim, n_class, input_droprate, hidden_droprate, batchnorm)\n",
    "        \n",
    "        self.dropout = node_dropout\n",
    "        self.node_dropout = nn.Dropout(node_dropout)\n",
    "\n",
    "    def forward(self, graph, feats, training = True):\n",
    "        \n",
    "        X = feats\n",
    "        S = self.S\n",
    "        \n",
    "        if training: # Training Mode\n",
    "            output_list = []\n",
    "            for s in range(S):\n",
    "                drop_feat = drop_node(X, self.dropout, True)  # Drop node\n",
    "                feat = GRANDConv(graph, drop_feat, self.K)    # Graph Convolution\n",
    "                output_list.append(th.log_softmax(self.mlp(feat), dim=-1))  # Prediction\n",
    "        \n",
    "            return output_list\n",
    "        else:   # Inference Mode\n",
    "            drop_feat = drop_node(X, self.dropout, False) \n",
    "            X =  GRANDConv(graph, drop_feat, self.K)\n",
    "\n",
    "            return th.log_softmax(self.mlp(X), dim = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training epochs.\n",
    "nb_epochs = 200\n",
    "# Patient epochs to wait before early stopping.\n",
    "early_stopping = 200\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "# L2 reg.\n",
    "weight_decay = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden layer dimensionalities\n",
    "hid_dim = 32\n",
    "# Dropnode rate (1 - keep probability).\n",
    "dropnode_rate = 0.5\n",
    "# Dropout rate of input layer\n",
    "input_droprate = 0.0\n",
    "# Dropout rate of hidden layer\n",
    "hidden_droprate = 0.0\n",
    "# Propagation step\n",
    "order = 8\n",
    "# Sampling times of dropnode\n",
    "sample = 4\n",
    "# Sharpening temperature\n",
    "tem = 0.5\n",
    "# Coefficient of consistency regularization\n",
    "lam = 1.0\n",
    "# Using Batch Normalization\n",
    "use_bn = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consis_loss(logps, temp, lam):\n",
    "    ps = [th.exp(p) for p in logps]\n",
    "    ps = th.stack(ps, dim = 2)\n",
    "    \n",
    "    avg_p = th.mean(ps, dim = 2)\n",
    "    sharp_p = (th.pow(avg_p, 1./temp) / th.sum(th.pow(avg_p, 1./temp), dim=1, keepdim=True)).detach()\n",
    "\n",
    "    sharp_p = sharp_p.unsqueeze(2)\n",
    "    loss = th.mean(th.sum(th.pow(ps - sharp_p, 2), dim = 1, keepdim=True))\n",
    "\n",
    "    loss = lam * loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(dataset):\n",
    "    graph = dataset[0]\n",
    "    graph = dgl.add_self_loop(graph)\n",
    "    \n",
    "    # retrieve the number of classes\n",
    "    n_classes = dataset.num_classes\n",
    "\n",
    "    # retrieve labels of ground truth\n",
    "    labels = graph.ndata.pop('label').to(device).long()\n",
    "    \n",
    "    # Extract node features\n",
    "    feats = graph.ndata.pop('feat').to(device)\n",
    "    n_features = feats.shape[-1]\n",
    "    \n",
    "    # retrieve masks for train/validation/test\n",
    "    train_mask = graph.ndata.pop('train_mask')\n",
    "    val_mask = graph.ndata.pop('val_mask')\n",
    "    test_mask = graph.ndata.pop('test_mask')\n",
    "    \n",
    "    train_idx = th.nonzero(train_mask, as_tuple=False).squeeze().to(device)\n",
    "    val_idx = th.nonzero(val_mask, as_tuple=False).squeeze().to(device)\n",
    "    test_idx = th.nonzero(test_mask, as_tuple=False).squeeze().to(device)\n",
    "    \n",
    "    return graph, n_classes, labels, feats, n_features, train_idx, val_idx, test_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'CoraGraphDataset'\n",
    "graph, n_classes, labels, feats, n_features, train_idx, val_idx, test_idx = data_split(datasets[dataname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = GRAND(n_features, hid_dim, n_classes, sample, order,\n",
    "                  dropnode_rate, input_droprate, \n",
    "                  hidden_droprate, use_bn)\n",
    "\n",
    "model = model.to(device)\n",
    "graph = graph.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training components\n",
    "loss_fn = nn.NLLLoss()\n",
    "opt = optim.Adam(model.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "\n",
    "loss_best = np.inf\n",
    "acc_best = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, Train Acc: 0.1500 | Train Loss: 1.9560 ,Val Acc: 0.1240 | Val Loss: 1.9221\n",
      "In epoch 1, Train Acc: 0.1571 | Train Loss: 1.9529 ,Val Acc: 0.3660 | Val Loss: 1.9244\n",
      "In epoch 2, Train Acc: 0.2786 | Train Loss: 1.9499 ,Val Acc: 0.3560 | Val Loss: 1.9265\n",
      "In epoch 3, Train Acc: 0.2214 | Train Loss: 1.9467 ,Val Acc: 0.3420 | Val Loss: 1.9289\n",
      "In epoch 4, Train Acc: 0.2000 | Train Loss: 1.9436 ,Val Acc: 0.3400 | Val Loss: 1.9309\n",
      "In epoch 5, Train Acc: 0.1929 | Train Loss: 1.9398 ,Val Acc: 0.3340 | Val Loss: 1.9329\n",
      "In epoch 6, Train Acc: 0.1857 | Train Loss: 1.9367 ,Val Acc: 0.3400 | Val Loss: 1.9351\n",
      "In epoch 7, Train Acc: 0.2857 | Train Loss: 1.9328 ,Val Acc: 0.3160 | Val Loss: 1.9372\n",
      "In epoch 8, Train Acc: 0.4143 | Train Loss: 1.9294 ,Val Acc: 0.1640 | Val Loss: 1.9389\n",
      "In epoch 9, Train Acc: 0.3714 | Train Loss: 1.9249 ,Val Acc: 0.1180 | Val Loss: 1.9399\n",
      "In epoch 10, Train Acc: 0.2071 | Train Loss: 1.9229 ,Val Acc: 0.1160 | Val Loss: 1.9399\n",
      "In epoch 11, Train Acc: 0.2357 | Train Loss: 1.9177 ,Val Acc: 0.1160 | Val Loss: 1.9390\n",
      "In epoch 12, Train Acc: 0.1929 | Train Loss: 1.9151 ,Val Acc: 0.1300 | Val Loss: 1.9372\n",
      "In epoch 13, Train Acc: 0.2357 | Train Loss: 1.9128 ,Val Acc: 0.1960 | Val Loss: 1.9344\n",
      "In epoch 14, Train Acc: 0.3714 | Train Loss: 1.9032 ,Val Acc: 0.3440 | Val Loss: 1.9309\n",
      "In epoch 15, Train Acc: 0.4643 | Train Loss: 1.8984 ,Val Acc: 0.4180 | Val Loss: 1.9273\n",
      "In epoch 16, Train Acc: 0.5429 | Train Loss: 1.8939 ,Val Acc: 0.4660 | Val Loss: 1.9232\n",
      "In epoch 17, Train Acc: 0.5929 | Train Loss: 1.8838 ,Val Acc: 0.4880 | Val Loss: 1.9187\n",
      "In epoch 18, Train Acc: 0.6500 | Train Loss: 1.8813 ,Val Acc: 0.4820 | Val Loss: 1.9145\n",
      "In epoch 19, Train Acc: 0.6357 | Train Loss: 1.8735 ,Val Acc: 0.4940 | Val Loss: 1.9100\n",
      "In epoch 20, Train Acc: 0.6857 | Train Loss: 1.8653 ,Val Acc: 0.5040 | Val Loss: 1.9049\n",
      "In epoch 21, Train Acc: 0.7286 | Train Loss: 1.8558 ,Val Acc: 0.5080 | Val Loss: 1.8993\n",
      "In epoch 22, Train Acc: 0.7929 | Train Loss: 1.8426 ,Val Acc: 0.5260 | Val Loss: 1.8932\n",
      "In epoch 23, Train Acc: 0.7929 | Train Loss: 1.8368 ,Val Acc: 0.5720 | Val Loss: 1.8868\n",
      "In epoch 24, Train Acc: 0.8214 | Train Loss: 1.8223 ,Val Acc: 0.6060 | Val Loss: 1.8800\n",
      "In epoch 25, Train Acc: 0.8357 | Train Loss: 1.8199 ,Val Acc: 0.6500 | Val Loss: 1.8729\n",
      "In epoch 26, Train Acc: 0.8357 | Train Loss: 1.8096 ,Val Acc: 0.6720 | Val Loss: 1.8656\n",
      "In epoch 27, Train Acc: 0.8143 | Train Loss: 1.8046 ,Val Acc: 0.6780 | Val Loss: 1.8577\n",
      "In epoch 28, Train Acc: 0.8500 | Train Loss: 1.7747 ,Val Acc: 0.6660 | Val Loss: 1.8492\n",
      "In epoch 29, Train Acc: 0.8786 | Train Loss: 1.7714 ,Val Acc: 0.6340 | Val Loss: 1.8411\n",
      "In epoch 30, Train Acc: 0.8286 | Train Loss: 1.7626 ,Val Acc: 0.6360 | Val Loss: 1.8336\n",
      "In epoch 31, Train Acc: 0.8286 | Train Loss: 1.7488 ,Val Acc: 0.6580 | Val Loss: 1.8267\n",
      "In epoch 32, Train Acc: 0.8643 | Train Loss: 1.7283 ,Val Acc: 0.6700 | Val Loss: 1.8200\n",
      "In epoch 33, Train Acc: 0.8357 | Train Loss: 1.7256 ,Val Acc: 0.6880 | Val Loss: 1.8122\n",
      "In epoch 34, Train Acc: 0.8571 | Train Loss: 1.7008 ,Val Acc: 0.7080 | Val Loss: 1.8023\n",
      "In epoch 35, Train Acc: 0.8214 | Train Loss: 1.6860 ,Val Acc: 0.7160 | Val Loss: 1.7905\n",
      "In epoch 36, Train Acc: 0.8643 | Train Loss: 1.6619 ,Val Acc: 0.7240 | Val Loss: 1.7775\n",
      "In epoch 37, Train Acc: 0.9000 | Train Loss: 1.6556 ,Val Acc: 0.7340 | Val Loss: 1.7644\n",
      "In epoch 38, Train Acc: 0.8500 | Train Loss: 1.6200 ,Val Acc: 0.7500 | Val Loss: 1.7502\n",
      "In epoch 39, Train Acc: 0.8643 | Train Loss: 1.6174 ,Val Acc: 0.7700 | Val Loss: 1.7379\n",
      "In epoch 40, Train Acc: 0.8857 | Train Loss: 1.6073 ,Val Acc: 0.7700 | Val Loss: 1.7263\n",
      "In epoch 41, Train Acc: 0.8714 | Train Loss: 1.5959 ,Val Acc: 0.7780 | Val Loss: 1.7167\n",
      "In epoch 42, Train Acc: 0.8714 | Train Loss: 1.5683 ,Val Acc: 0.7820 | Val Loss: 1.7065\n",
      "In epoch 43, Train Acc: 0.9143 | Train Loss: 1.5466 ,Val Acc: 0.7740 | Val Loss: 1.6947\n",
      "In epoch 44, Train Acc: 0.8786 | Train Loss: 1.5425 ,Val Acc: 0.7680 | Val Loss: 1.6808\n",
      "In epoch 45, Train Acc: 0.8786 | Train Loss: 1.5083 ,Val Acc: 0.7580 | Val Loss: 1.6658\n",
      "In epoch 46, Train Acc: 0.8429 | Train Loss: 1.5062 ,Val Acc: 0.7460 | Val Loss: 1.6514\n",
      "In epoch 47, Train Acc: 0.8643 | Train Loss: 1.4869 ,Val Acc: 0.7480 | Val Loss: 1.6393\n",
      "In epoch 48, Train Acc: 0.8143 | Train Loss: 1.4683 ,Val Acc: 0.7580 | Val Loss: 1.6271\n",
      "In epoch 49, Train Acc: 0.8571 | Train Loss: 1.4373 ,Val Acc: 0.7640 | Val Loss: 1.6135\n",
      "In epoch 50, Train Acc: 0.9071 | Train Loss: 1.4276 ,Val Acc: 0.7580 | Val Loss: 1.5993\n",
      "In epoch 51, Train Acc: 0.8929 | Train Loss: 1.4106 ,Val Acc: 0.7700 | Val Loss: 1.5828\n",
      "In epoch 52, Train Acc: 0.8786 | Train Loss: 1.3994 ,Val Acc: 0.7960 | Val Loss: 1.5668\n",
      "In epoch 53, Train Acc: 0.8929 | Train Loss: 1.3578 ,Val Acc: 0.7960 | Val Loss: 1.5501\n",
      "In epoch 54, Train Acc: 0.8714 | Train Loss: 1.3713 ,Val Acc: 0.7980 | Val Loss: 1.5369\n",
      "In epoch 55, Train Acc: 0.8643 | Train Loss: 1.3576 ,Val Acc: 0.7920 | Val Loss: 1.5243\n",
      "In epoch 56, Train Acc: 0.8786 | Train Loss: 1.3176 ,Val Acc: 0.7920 | Val Loss: 1.5115\n",
      "In epoch 57, Train Acc: 0.9071 | Train Loss: 1.3245 ,Val Acc: 0.7960 | Val Loss: 1.4923\n",
      "In epoch 58, Train Acc: 0.9357 | Train Loss: 1.2865 ,Val Acc: 0.8080 | Val Loss: 1.4730\n",
      "In epoch 59, Train Acc: 0.8786 | Train Loss: 1.2820 ,Val Acc: 0.8060 | Val Loss: 1.4555\n",
      "In epoch 60, Train Acc: 0.9143 | Train Loss: 1.2490 ,Val Acc: 0.8020 | Val Loss: 1.4419\n",
      "In epoch 61, Train Acc: 0.9000 | Train Loss: 1.2231 ,Val Acc: 0.7980 | Val Loss: 1.4342\n",
      "In epoch 62, Train Acc: 0.8929 | Train Loss: 1.2294 ,Val Acc: 0.7860 | Val Loss: 1.4277\n",
      "In epoch 63, Train Acc: 0.9000 | Train Loss: 1.2290 ,Val Acc: 0.7860 | Val Loss: 1.4167\n",
      "In epoch 64, Train Acc: 0.8857 | Train Loss: 1.1979 ,Val Acc: 0.7940 | Val Loss: 1.3958\n",
      "In epoch 65, Train Acc: 0.8929 | Train Loss: 1.1699 ,Val Acc: 0.8020 | Val Loss: 1.3721\n",
      "In epoch 66, Train Acc: 0.8786 | Train Loss: 1.1764 ,Val Acc: 0.8040 | Val Loss: 1.3510\n",
      "In epoch 67, Train Acc: 0.9286 | Train Loss: 1.1318 ,Val Acc: 0.8060 | Val Loss: 1.3352\n",
      "In epoch 68, Train Acc: 0.8714 | Train Loss: 1.1716 ,Val Acc: 0.8000 | Val Loss: 1.3246\n",
      "In epoch 69, Train Acc: 0.8857 | Train Loss: 1.1379 ,Val Acc: 0.7960 | Val Loss: 1.3140\n",
      "In epoch 70, Train Acc: 0.8714 | Train Loss: 1.0972 ,Val Acc: 0.8040 | Val Loss: 1.3016\n",
      "In epoch 71, Train Acc: 0.8929 | Train Loss: 1.0830 ,Val Acc: 0.8140 | Val Loss: 1.2825\n",
      "In epoch 72, Train Acc: 0.9143 | Train Loss: 1.0576 ,Val Acc: 0.8160 | Val Loss: 1.2641\n",
      "In epoch 73, Train Acc: 0.8786 | Train Loss: 1.0976 ,Val Acc: 0.8140 | Val Loss: 1.2537\n",
      "In epoch 74, Train Acc: 0.9071 | Train Loss: 1.0515 ,Val Acc: 0.8000 | Val Loss: 1.2455\n",
      "In epoch 75, Train Acc: 0.8929 | Train Loss: 1.0564 ,Val Acc: 0.8000 | Val Loss: 1.2362\n",
      "In epoch 76, Train Acc: 0.8929 | Train Loss: 1.0743 ,Val Acc: 0.7980 | Val Loss: 1.2239\n",
      "In epoch 77, Train Acc: 0.8929 | Train Loss: 1.0524 ,Val Acc: 0.8000 | Val Loss: 1.2091\n",
      "In epoch 78, Train Acc: 0.8857 | Train Loss: 1.0177 ,Val Acc: 0.8060 | Val Loss: 1.1941\n",
      "In epoch 79, Train Acc: 0.8714 | Train Loss: 1.0473 ,Val Acc: 0.8100 | Val Loss: 1.1782\n",
      "In epoch 80, Train Acc: 0.8643 | Train Loss: 1.0168 ,Val Acc: 0.8140 | Val Loss: 1.1681\n",
      "In epoch 81, Train Acc: 0.8929 | Train Loss: 0.9812 ,Val Acc: 0.8100 | Val Loss: 1.1597\n",
      "In epoch 82, Train Acc: 0.9000 | Train Loss: 0.9786 ,Val Acc: 0.8040 | Val Loss: 1.1540\n",
      "In epoch 83, Train Acc: 0.9214 | Train Loss: 0.9775 ,Val Acc: 0.7980 | Val Loss: 1.1442\n",
      "In epoch 84, Train Acc: 0.8929 | Train Loss: 0.9638 ,Val Acc: 0.8000 | Val Loss: 1.1282\n",
      "In epoch 85, Train Acc: 0.9000 | Train Loss: 0.9715 ,Val Acc: 0.8000 | Val Loss: 1.1151\n",
      "In epoch 86, Train Acc: 0.9000 | Train Loss: 0.9735 ,Val Acc: 0.8060 | Val Loss: 1.1076\n",
      "In epoch 87, Train Acc: 0.8857 | Train Loss: 0.9502 ,Val Acc: 0.8040 | Val Loss: 1.1032\n",
      "In epoch 88, Train Acc: 0.8857 | Train Loss: 0.9365 ,Val Acc: 0.8020 | Val Loss: 1.0961\n",
      "In epoch 89, Train Acc: 0.9357 | Train Loss: 0.9322 ,Val Acc: 0.8020 | Val Loss: 1.0825\n",
      "In epoch 90, Train Acc: 0.9214 | Train Loss: 0.9189 ,Val Acc: 0.8060 | Val Loss: 1.0665\n",
      "In epoch 91, Train Acc: 0.9143 | Train Loss: 0.8971 ,Val Acc: 0.8060 | Val Loss: 1.0503\n",
      "In epoch 92, Train Acc: 0.9000 | Train Loss: 0.8836 ,Val Acc: 0.8060 | Val Loss: 1.0404\n",
      "In epoch 93, Train Acc: 0.8786 | Train Loss: 0.9047 ,Val Acc: 0.8060 | Val Loss: 1.0361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 94, Train Acc: 0.9143 | Train Loss: 0.8763 ,Val Acc: 0.8140 | Val Loss: 1.0345\n",
      "In epoch 95, Train Acc: 0.9143 | Train Loss: 0.8494 ,Val Acc: 0.8060 | Val Loss: 1.0308\n",
      "In epoch 96, Train Acc: 0.9214 | Train Loss: 0.9062 ,Val Acc: 0.8040 | Val Loss: 1.0296\n",
      "In epoch 97, Train Acc: 0.9143 | Train Loss: 0.8921 ,Val Acc: 0.8040 | Val Loss: 1.0215\n",
      "In epoch 98, Train Acc: 0.9000 | Train Loss: 0.8479 ,Val Acc: 0.8040 | Val Loss: 1.0118\n",
      "In epoch 99, Train Acc: 0.8714 | Train Loss: 0.8712 ,Val Acc: 0.8080 | Val Loss: 1.0036\n",
      "In epoch 100, Train Acc: 0.8929 | Train Loss: 0.8407 ,Val Acc: 0.8080 | Val Loss: 0.9947\n",
      "In epoch 101, Train Acc: 0.8857 | Train Loss: 0.8721 ,Val Acc: 0.8080 | Val Loss: 0.9863\n",
      "In epoch 102, Train Acc: 0.8714 | Train Loss: 0.8258 ,Val Acc: 0.8040 | Val Loss: 0.9816\n",
      "In epoch 103, Train Acc: 0.8643 | Train Loss: 0.8406 ,Val Acc: 0.8100 | Val Loss: 0.9811\n",
      "In epoch 104, Train Acc: 0.9357 | Train Loss: 0.7904 ,Val Acc: 0.8060 | Val Loss: 0.9721\n",
      "In epoch 105, Train Acc: 0.8786 | Train Loss: 0.8380 ,Val Acc: 0.8140 | Val Loss: 0.9596\n",
      "In epoch 106, Train Acc: 0.9143 | Train Loss: 0.8330 ,Val Acc: 0.8100 | Val Loss: 0.9518\n",
      "In epoch 107, Train Acc: 0.9286 | Train Loss: 0.7904 ,Val Acc: 0.8100 | Val Loss: 0.9466\n",
      "In epoch 108, Train Acc: 0.9071 | Train Loss: 0.8112 ,Val Acc: 0.8120 | Val Loss: 0.9463\n",
      "In epoch 109, Train Acc: 0.9143 | Train Loss: 0.7729 ,Val Acc: 0.8140 | Val Loss: 0.9448\n",
      "In epoch 110, Train Acc: 0.9000 | Train Loss: 0.8256 ,Val Acc: 0.8080 | Val Loss: 0.9445\n",
      "In epoch 111, Train Acc: 0.9357 | Train Loss: 0.8076 ,Val Acc: 0.8080 | Val Loss: 0.9419\n",
      "In epoch 112, Train Acc: 0.9143 | Train Loss: 0.7770 ,Val Acc: 0.8060 | Val Loss: 0.9350\n",
      "In epoch 113, Train Acc: 0.8929 | Train Loss: 0.7730 ,Val Acc: 0.8120 | Val Loss: 0.9242\n",
      "In epoch 114, Train Acc: 0.9143 | Train Loss: 0.7917 ,Val Acc: 0.8160 | Val Loss: 0.9121\n",
      "In epoch 115, Train Acc: 0.9214 | Train Loss: 0.7684 ,Val Acc: 0.8100 | Val Loss: 0.8999\n",
      "In epoch 116, Train Acc: 0.9071 | Train Loss: 0.7686 ,Val Acc: 0.8060 | Val Loss: 0.8977\n",
      "In epoch 117, Train Acc: 0.9429 | Train Loss: 0.7594 ,Val Acc: 0.8040 | Val Loss: 0.9032\n",
      "In epoch 118, Train Acc: 0.9357 | Train Loss: 0.7540 ,Val Acc: 0.8120 | Val Loss: 0.9093\n",
      "In epoch 119, Train Acc: 0.9214 | Train Loss: 0.7617 ,Val Acc: 0.8080 | Val Loss: 0.9160\n",
      "In epoch 120, Train Acc: 0.8857 | Train Loss: 0.7808 ,Val Acc: 0.8040 | Val Loss: 0.9165\n",
      "In epoch 121, Train Acc: 0.9214 | Train Loss: 0.7706 ,Val Acc: 0.8120 | Val Loss: 0.9005\n",
      "In epoch 122, Train Acc: 0.9571 | Train Loss: 0.7396 ,Val Acc: 0.8100 | Val Loss: 0.8804\n",
      "In epoch 123, Train Acc: 0.9143 | Train Loss: 0.7288 ,Val Acc: 0.8140 | Val Loss: 0.8730\n",
      "In epoch 124, Train Acc: 0.9143 | Train Loss: 0.7595 ,Val Acc: 0.8120 | Val Loss: 0.8725\n",
      "In epoch 125, Train Acc: 0.9286 | Train Loss: 0.7428 ,Val Acc: 0.8140 | Val Loss: 0.8735\n",
      "In epoch 126, Train Acc: 0.9214 | Train Loss: 0.7208 ,Val Acc: 0.8120 | Val Loss: 0.8744\n",
      "In epoch 127, Train Acc: 0.9071 | Train Loss: 0.7148 ,Val Acc: 0.8080 | Val Loss: 0.8793\n",
      "In epoch 128, Train Acc: 0.9071 | Train Loss: 0.7346 ,Val Acc: 0.8060 | Val Loss: 0.8812\n",
      "In epoch 129, Train Acc: 0.9000 | Train Loss: 0.7483 ,Val Acc: 0.8100 | Val Loss: 0.8745\n",
      "In epoch 130, Train Acc: 0.9429 | Train Loss: 0.7502 ,Val Acc: 0.8120 | Val Loss: 0.8652\n",
      "In epoch 131, Train Acc: 0.9143 | Train Loss: 0.6936 ,Val Acc: 0.8180 | Val Loss: 0.8549\n",
      "In epoch 132, Train Acc: 0.9000 | Train Loss: 0.7272 ,Val Acc: 0.8180 | Val Loss: 0.8523\n",
      "In epoch 133, Train Acc: 0.9071 | Train Loss: 0.7073 ,Val Acc: 0.8200 | Val Loss: 0.8497\n",
      "In epoch 134, Train Acc: 0.9500 | Train Loss: 0.7146 ,Val Acc: 0.8180 | Val Loss: 0.8528\n",
      "In epoch 135, Train Acc: 0.9357 | Train Loss: 0.7252 ,Val Acc: 0.8100 | Val Loss: 0.8547\n",
      "In epoch 136, Train Acc: 0.9429 | Train Loss: 0.6791 ,Val Acc: 0.8120 | Val Loss: 0.8512\n",
      "In epoch 137, Train Acc: 0.9357 | Train Loss: 0.7046 ,Val Acc: 0.8080 | Val Loss: 0.8479\n",
      "In epoch 138, Train Acc: 0.9214 | Train Loss: 0.6556 ,Val Acc: 0.8040 | Val Loss: 0.8449\n",
      "In epoch 139, Train Acc: 0.9143 | Train Loss: 0.7178 ,Val Acc: 0.8040 | Val Loss: 0.8457\n",
      "In epoch 140, Train Acc: 0.9286 | Train Loss: 0.6649 ,Val Acc: 0.8120 | Val Loss: 0.8477\n",
      "In epoch 141, Train Acc: 0.9286 | Train Loss: 0.6878 ,Val Acc: 0.8200 | Val Loss: 0.8473\n",
      "In epoch 142, Train Acc: 0.9500 | Train Loss: 0.6872 ,Val Acc: 0.8140 | Val Loss: 0.8410\n",
      "In epoch 143, Train Acc: 0.9286 | Train Loss: 0.6860 ,Val Acc: 0.8180 | Val Loss: 0.8348\n",
      "In epoch 144, Train Acc: 0.9643 | Train Loss: 0.6831 ,Val Acc: 0.8080 | Val Loss: 0.8337\n",
      "In epoch 145, Train Acc: 0.9357 | Train Loss: 0.6733 ,Val Acc: 0.8060 | Val Loss: 0.8318\n",
      "In epoch 146, Train Acc: 0.9286 | Train Loss: 0.6929 ,Val Acc: 0.8100 | Val Loss: 0.8314\n",
      "In epoch 147, Train Acc: 0.9000 | Train Loss: 0.6834 ,Val Acc: 0.8060 | Val Loss: 0.8306\n",
      "In epoch 148, Train Acc: 0.8929 | Train Loss: 0.6797 ,Val Acc: 0.8080 | Val Loss: 0.8313\n",
      "In epoch 149, Train Acc: 0.9429 | Train Loss: 0.6551 ,Val Acc: 0.8140 | Val Loss: 0.8263\n",
      "In epoch 150, Train Acc: 0.9500 | Train Loss: 0.6511 ,Val Acc: 0.8180 | Val Loss: 0.8200\n",
      "In epoch 151, Train Acc: 0.9286 | Train Loss: 0.6491 ,Val Acc: 0.8160 | Val Loss: 0.8190\n",
      "In epoch 152, Train Acc: 0.9286 | Train Loss: 0.6660 ,Val Acc: 0.8100 | Val Loss: 0.8195\n",
      "In epoch 153, Train Acc: 0.9214 | Train Loss: 0.6767 ,Val Acc: 0.8080 | Val Loss: 0.8194\n",
      "In epoch 154, Train Acc: 0.9357 | Train Loss: 0.6475 ,Val Acc: 0.8100 | Val Loss: 0.8128\n",
      "In epoch 155, Train Acc: 0.9286 | Train Loss: 0.6295 ,Val Acc: 0.8100 | Val Loss: 0.7999\n",
      "In epoch 156, Train Acc: 0.9357 | Train Loss: 0.6514 ,Val Acc: 0.8060 | Val Loss: 0.7975\n",
      "In epoch 157, Train Acc: 0.9286 | Train Loss: 0.6356 ,Val Acc: 0.8040 | Val Loss: 0.7996\n",
      "In epoch 158, Train Acc: 0.9571 | Train Loss: 0.6502 ,Val Acc: 0.8100 | Val Loss: 0.8071\n",
      "In epoch 159, Train Acc: 0.9571 | Train Loss: 0.6585 ,Val Acc: 0.8100 | Val Loss: 0.8180\n",
      "In epoch 160, Train Acc: 0.9071 | Train Loss: 0.6768 ,Val Acc: 0.8020 | Val Loss: 0.8223\n",
      "In epoch 161, Train Acc: 0.9500 | Train Loss: 0.6400 ,Val Acc: 0.7980 | Val Loss: 0.8184\n",
      "In epoch 162, Train Acc: 0.9071 | Train Loss: 0.6402 ,Val Acc: 0.8020 | Val Loss: 0.8078\n",
      "In epoch 163, Train Acc: 0.9143 | Train Loss: 0.6431 ,Val Acc: 0.8080 | Val Loss: 0.7926\n",
      "In epoch 164, Train Acc: 0.9286 | Train Loss: 0.6089 ,Val Acc: 0.8080 | Val Loss: 0.7814\n",
      "In epoch 165, Train Acc: 0.8929 | Train Loss: 0.6504 ,Val Acc: 0.8080 | Val Loss: 0.7804\n",
      "In epoch 166, Train Acc: 0.9286 | Train Loss: 0.6221 ,Val Acc: 0.8020 | Val Loss: 0.7867\n",
      "In epoch 167, Train Acc: 0.9286 | Train Loss: 0.6480 ,Val Acc: 0.8000 | Val Loss: 0.7954\n",
      "In epoch 168, Train Acc: 0.9000 | Train Loss: 0.6405 ,Val Acc: 0.8020 | Val Loss: 0.8047\n",
      "In epoch 169, Train Acc: 0.9357 | Train Loss: 0.6637 ,Val Acc: 0.8120 | Val Loss: 0.8016\n",
      "In epoch 170, Train Acc: 0.9357 | Train Loss: 0.6324 ,Val Acc: 0.8080 | Val Loss: 0.7913\n",
      "In epoch 171, Train Acc: 0.9286 | Train Loss: 0.6237 ,Val Acc: 0.8100 | Val Loss: 0.7811\n",
      "In epoch 172, Train Acc: 0.9214 | Train Loss: 0.6442 ,Val Acc: 0.8100 | Val Loss: 0.7773\n",
      "In epoch 173, Train Acc: 0.8929 | Train Loss: 0.6288 ,Val Acc: 0.8100 | Val Loss: 0.7764\n",
      "In epoch 174, Train Acc: 0.9286 | Train Loss: 0.5951 ,Val Acc: 0.8100 | Val Loss: 0.7806\n",
      "In epoch 175, Train Acc: 0.9643 | Train Loss: 0.5981 ,Val Acc: 0.8100 | Val Loss: 0.7843\n",
      "In epoch 176, Train Acc: 0.9429 | Train Loss: 0.6276 ,Val Acc: 0.8120 | Val Loss: 0.7832\n",
      "In epoch 177, Train Acc: 0.9357 | Train Loss: 0.5988 ,Val Acc: 0.8120 | Val Loss: 0.7779\n",
      "In epoch 178, Train Acc: 0.9071 | Train Loss: 0.6241 ,Val Acc: 0.8120 | Val Loss: 0.7668\n",
      "In epoch 179, Train Acc: 0.9143 | Train Loss: 0.6060 ,Val Acc: 0.8120 | Val Loss: 0.7608\n",
      "In epoch 180, Train Acc: 0.9286 | Train Loss: 0.6068 ,Val Acc: 0.8080 | Val Loss: 0.7622\n",
      "In epoch 181, Train Acc: 0.9143 | Train Loss: 0.6261 ,Val Acc: 0.8060 | Val Loss: 0.7734\n",
      "In epoch 182, Train Acc: 0.9071 | Train Loss: 0.6390 ,Val Acc: 0.8040 | Val Loss: 0.7936\n",
      "In epoch 183, Train Acc: 0.9571 | Train Loss: 0.5878 ,Val Acc: 0.8040 | Val Loss: 0.8003\n",
      "In epoch 184, Train Acc: 0.9571 | Train Loss: 0.5918 ,Val Acc: 0.8100 | Val Loss: 0.7925\n",
      "In epoch 185, Train Acc: 0.8857 | Train Loss: 0.5992 ,Val Acc: 0.8100 | Val Loss: 0.7750\n",
      "In epoch 186, Train Acc: 0.9500 | Train Loss: 0.6133 ,Val Acc: 0.8100 | Val Loss: 0.7615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 187, Train Acc: 0.9071 | Train Loss: 0.6089 ,Val Acc: 0.8160 | Val Loss: 0.7538\n",
      "In epoch 188, Train Acc: 0.9143 | Train Loss: 0.6054 ,Val Acc: 0.8160 | Val Loss: 0.7547\n",
      "In epoch 189, Train Acc: 0.9500 | Train Loss: 0.5848 ,Val Acc: 0.8120 | Val Loss: 0.7589\n",
      "In epoch 190, Train Acc: 0.9429 | Train Loss: 0.6204 ,Val Acc: 0.8140 | Val Loss: 0.7643\n",
      "In epoch 191, Train Acc: 0.9286 | Train Loss: 0.6115 ,Val Acc: 0.8120 | Val Loss: 0.7717\n",
      "In epoch 192, Train Acc: 0.9357 | Train Loss: 0.6028 ,Val Acc: 0.8100 | Val Loss: 0.7753\n",
      "In epoch 193, Train Acc: 0.9429 | Train Loss: 0.5918 ,Val Acc: 0.8080 | Val Loss: 0.7719\n",
      "In epoch 194, Train Acc: 0.9000 | Train Loss: 0.6188 ,Val Acc: 0.8100 | Val Loss: 0.7651\n",
      "In epoch 195, Train Acc: 0.9357 | Train Loss: 0.6104 ,Val Acc: 0.8100 | Val Loss: 0.7569\n",
      "In epoch 196, Train Acc: 0.9286 | Train Loss: 0.5956 ,Val Acc: 0.8100 | Val Loss: 0.7498\n",
      "In epoch 197, Train Acc: 0.9214 | Train Loss: 0.5876 ,Val Acc: 0.8160 | Val Loss: 0.7522\n",
      "In epoch 198, Train Acc: 0.9286 | Train Loss: 0.5652 ,Val Acc: 0.8140 | Val Loss: 0.7567\n",
      "In epoch 199, Train Acc: 0.9429 | Train Loss: 0.5843 ,Val Acc: 0.8100 | Val Loss: 0.7591\n",
      "Optimization Finished!\n",
      "Loading 196th epoch\n",
      "Test Acc: 0.8290\n"
     ]
    }
   ],
   "source": [
    "# training epoches\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    ''' Training '''\n",
    "    model.train()\n",
    "\n",
    "    loss_sup = 0\n",
    "    logits = model(graph, feats, True)\n",
    "\n",
    "    # calculate supervised loss\n",
    "    for k in range(sample):\n",
    "        loss_sup += F.nll_loss(logits[k][train_idx], labels[train_idx])\n",
    "\n",
    "    loss_sup = loss_sup / sample\n",
    "\n",
    "    # calculate consistency loss\n",
    "    loss_consis = consis_loss(logits, tem, lam)\n",
    "\n",
    "    loss_train = loss_sup + loss_consis\n",
    "    acc_train = th.sum(logits[0][train_idx].argmax(dim=1) == labels[train_idx]).item() / len(train_idx)\n",
    "\n",
    "    # backward\n",
    "    opt.zero_grad()\n",
    "    loss_train.backward()\n",
    "    opt.step()\n",
    "\n",
    "    ''' Validating '''\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "\n",
    "        val_logits = model(graph, feats, False)\n",
    "\n",
    "        loss_val = F.nll_loss(val_logits[val_idx], labels[val_idx])\n",
    "        acc_val = th.sum(val_logits[val_idx].argmax(dim=1) == labels[val_idx]).item() / len(val_idx)\n",
    "\n",
    "        # Print out performance\n",
    "        print(\"In epoch {}, Train Acc: {:.4f} | Train Loss: {:.4f} ,Val Acc: {:.4f} | Val Loss: {:.4f}\".\n",
    "              format(epoch, acc_train, loss_train.item(), acc_val, loss_val.item()))\n",
    "\n",
    "        # set early stopping counter\n",
    "        if loss_val < loss_best or acc_val > acc_best:\n",
    "            if loss_val < loss_best:\n",
    "                best_epoch = epoch\n",
    "                th.save(model.state_dict(), dataname + '.pkl')\n",
    "            no_improvement = 0\n",
    "            loss_best = min(loss_val, loss_best)\n",
    "            acc_best = max(acc_val, acc_best)\n",
    "        else:\n",
    "            no_improvement += 1\n",
    "            if no_improvement == early_stopping:\n",
    "                print('Early stopping.')\n",
    "                break\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(th.load(dataname + '.pkl'))\n",
    "\n",
    "''' Testing '''\n",
    "model.eval()\n",
    "\n",
    "test_logits = model(graph, feats, False)\n",
    "test_acc = th.sum(test_logits[test_idx].argmax(dim=1) == labels[test_idx]).item() / len(test_idx)\n",
    "\n",
    "print(\"Test Acc: {:.4f}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------\n",
    "# Part II : GraphCONV\n",
    "    - Dataset: CoauthorCSDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of categories: 15\n"
     ]
    }
   ],
   "source": [
    "dataset2 = dgl.data.CoauthorCSDataset()\n",
    "graph2 = dataset2[0]\n",
    "print('Number of categories:', dataset2.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import GraphConv\n",
    "\n",
    "# Le modèle de graphes convolution network utilisé\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConv(in_feats, h_feats)\n",
    "        self.conv2 = GraphConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g, model):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "\n",
    "    # On créé les masques pour créer les ensembles de train, test et validation\n",
    "    nb_nodes = len(g.ndata['feat'])\n",
    "    mask1 = int(nb_nodes*0.2)\n",
    "    mask2 = int(nb_nodes*0.7)\n",
    "    features = g.ndata['feat']\n",
    "    labels = g.ndata['label']\n",
    "    \n",
    "    train_mask = np.zeros(nb_nodes,dtype=bool)\n",
    "    train_mask[:mask1] = 1\n",
    "    train_mask = th.from_numpy(train_mask)\n",
    "    \n",
    "    val_mask = np.zeros(nb_nodes,dtype=bool)\n",
    "    val_mask[mask1:mask2] = 1\n",
    "    val_mask = th.from_numpy(val_mask)\n",
    "    \n",
    "    test_mask = np.zeros(nb_nodes,dtype=bool)\n",
    "    test_mask[mask2:] = 1\n",
    "    test_mask = th.from_numpy(test_mask)\n",
    "    \n",
    "    # train loop\n",
    "    for e in range(nb_epochs):\n",
    "        logits = model(g, features)\n",
    "\n",
    "        pred = logits.argmax(1)\n",
    "\n",
    "        # On calcule la loss\n",
    "        loss = F.cross_entropy(logits[train_mask], labels[train_mask])\n",
    "\n",
    "        # On calcule la précision sur chaque set\n",
    "        train_acc = (pred[train_mask] == labels[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == labels[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == labels[test_mask]).float().mean()\n",
    "\n",
    "        # On garde la meilleure validation accuracy\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if e % 5 == 0:\n",
    "            print('Epoque {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                e, loss, val_acc, best_val_acc, test_acc, best_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque 0, loss: 2.727, val acc: 0.120 (best 0.120), test acc: 0.056 (best 0.056)\n",
      "Epoque 5, loss: 0.824, val acc: 0.570 (best 0.570), test acc: 0.294 (best 0.294)\n",
      "Epoque 10, loss: 0.471, val acc: 0.732 (best 0.732), test acc: 0.459 (best 0.459)\n",
      "Epoque 15, loss: 0.278, val acc: 0.818 (best 0.818), test acc: 0.609 (best 0.609)\n",
      "Epoque 20, loss: 0.197, val acc: 0.843 (best 0.843), test acc: 0.608 (best 0.608)\n",
      "Epoque 25, loss: 0.148, val acc: 0.874 (best 0.874), test acc: 0.680 (best 0.680)\n",
      "Epoque 30, loss: 0.115, val acc: 0.891 (best 0.891), test acc: 0.726 (best 0.726)\n",
      "Epoque 35, loss: 0.094, val acc: 0.901 (best 0.901), test acc: 0.727 (best 0.727)\n",
      "Epoque 40, loss: 0.080, val acc: 0.898 (best 0.901), test acc: 0.723 (best 0.727)\n",
      "Epoque 45, loss: 0.067, val acc: 0.900 (best 0.901), test acc: 0.739 (best 0.727)\n",
      "Epoque 50, loss: 0.058, val acc: 0.899 (best 0.901), test acc: 0.745 (best 0.727)\n",
      "Epoque 55, loss: 0.051, val acc: 0.897 (best 0.901), test acc: 0.755 (best 0.727)\n",
      "Epoque 60, loss: 0.045, val acc: 0.899 (best 0.901), test acc: 0.779 (best 0.727)\n",
      "Epoque 65, loss: 0.040, val acc: 0.901 (best 0.901), test acc: 0.794 (best 0.727)\n",
      "Epoque 70, loss: 0.035, val acc: 0.899 (best 0.901), test acc: 0.803 (best 0.727)\n",
      "Epoque 75, loss: 0.032, val acc: 0.898 (best 0.901), test acc: 0.808 (best 0.727)\n",
      "Epoque 80, loss: 0.029, val acc: 0.898 (best 0.901), test acc: 0.812 (best 0.727)\n",
      "Epoque 85, loss: 0.027, val acc: 0.898 (best 0.901), test acc: 0.815 (best 0.727)\n",
      "Epoque 90, loss: 0.024, val acc: 0.898 (best 0.901), test acc: 0.818 (best 0.727)\n",
      "Epoque 95, loss: 0.022, val acc: 0.898 (best 0.901), test acc: 0.819 (best 0.727)\n",
      "Epoque 100, loss: 0.021, val acc: 0.897 (best 0.901), test acc: 0.820 (best 0.727)\n",
      "Epoque 105, loss: 0.019, val acc: 0.897 (best 0.901), test acc: 0.820 (best 0.727)\n",
      "Epoque 110, loss: 0.018, val acc: 0.896 (best 0.901), test acc: 0.820 (best 0.727)\n",
      "Epoque 115, loss: 0.017, val acc: 0.896 (best 0.901), test acc: 0.820 (best 0.727)\n",
      "Epoque 120, loss: 0.016, val acc: 0.896 (best 0.901), test acc: 0.819 (best 0.727)\n",
      "Epoque 125, loss: 0.015, val acc: 0.895 (best 0.901), test acc: 0.819 (best 0.727)\n",
      "Epoque 130, loss: 0.014, val acc: 0.895 (best 0.901), test acc: 0.818 (best 0.727)\n",
      "Epoque 135, loss: 0.013, val acc: 0.895 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 140, loss: 0.013, val acc: 0.895 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 145, loss: 0.012, val acc: 0.894 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 150, loss: 0.011, val acc: 0.894 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 155, loss: 0.011, val acc: 0.894 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 160, loss: 0.010, val acc: 0.893 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 165, loss: 0.010, val acc: 0.893 (best 0.901), test acc: 0.817 (best 0.727)\n",
      "Epoque 170, loss: 0.010, val acc: 0.893 (best 0.901), test acc: 0.818 (best 0.727)\n",
      "Epoque 175, loss: 0.009, val acc: 0.892 (best 0.901), test acc: 0.819 (best 0.727)\n",
      "Epoque 180, loss: 0.009, val acc: 0.892 (best 0.901), test acc: 0.819 (best 0.727)\n",
      "Epoque 185, loss: 0.009, val acc: 0.892 (best 0.901), test acc: 0.820 (best 0.727)\n",
      "Epoque 190, loss: 0.009, val acc: 0.892 (best 0.901), test acc: 0.821 (best 0.727)\n",
      "Epoque 195, loss: 0.008, val acc: 0.892 (best 0.901), test acc: 0.821 (best 0.727)\n"
     ]
    }
   ],
   "source": [
    "model2 = GCN(graph2.ndata['feat'].shape[1], 16, dataset2.num_classes)\n",
    "train(graph2, model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: Link Prediction\n",
    "   - Link Prediction avec : CoraGraphDataset\n",
    "       - Prediction de l'existance d'une arrête entre deux noeuds arbitraire !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "dataset3 = dgl.data.CoraGraphDataset()\n",
    "g3 = dataset3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On répartit les arêtes dans des ensembles de train et de test (20% des données en test ici)\n",
    "u, v = g3.edges()\n",
    "\n",
    "eids = np.arange(g3.number_of_edges())\n",
    "eids = np.random.permutation(eids)\n",
    "test_size = int(len(eids) * 0.2)\n",
    "train_size = g3.number_of_edges() - test_size\n",
    "test_pos_u, test_pos_v = u[eids[:test_size]], v[eids[:test_size]]\n",
    "train_pos_u, train_pos_v = u[eids[test_size:]], v[eids[test_size:]]\n",
    "\n",
    "adj = sp.coo_matrix((np.ones(len(u)), (u.numpy(), v.numpy())))\n",
    "adj_neg = 1 - adj.todense() - np.eye(g3.number_of_nodes())\n",
    "neg_u, neg_v = np.where(adj_neg != 0)\n",
    "\n",
    "# On sépare les noeuds de chaque arête\n",
    "neg_eids = np.random.choice(len(neg_u), g3.number_of_edges())\n",
    "test_neg_u, test_neg_v = neg_u[neg_eids[:test_size]], neg_v[neg_eids[:test_size]]\n",
    "train_neg_u, train_neg_v = neg_u[neg_eids[test_size:]], neg_v[neg_eids[test_size:]]\n",
    "\n",
    "# On crée également le graphe ne contenant que les arêtes de train (le graphe - les arêtes de test)\n",
    "train_g3 = dgl.remove_edges(g3, eids[:test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# On créé un modèle GrapheSage\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On créé 2 graphes pour chaque l'ensemble de train et de test, puisque on fait de la prédiction d'arêtes donc de paires de noeuds\n",
    "train_pos_g3 = dgl.graph((train_pos_u, train_pos_v), num_nodes=g3.number_of_nodes())\n",
    "train_neg_g3 = dgl.graph((train_neg_u, train_neg_v), num_nodes=g3.number_of_nodes())\n",
    "\n",
    "test_pos_g3 = dgl.graph((test_pos_u, test_pos_v), num_nodes=g3.number_of_nodes())\n",
    "test_neg_g3 = dgl.graph((test_neg_u, test_neg_v), num_nodes=g3.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # On créé une nouvelle arête entre 2 neouds de feature 'h'\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphSAGE(train_g3.ndata['feat'].shape[1], 16)\n",
    "pred = DotPredictor()\n",
    "\n",
    "# Calcul de la perte\n",
    "def compute_loss(pos_score, neg_score):\n",
    "    scores = th.cat([pos_score, neg_score])\n",
    "    labels = th.cat([th.ones(pos_score.shape[0]), th.zeros(neg_score.shape[0])])\n",
    "    return F.binary_cross_entropy_with_logits(scores, labels)\n",
    "\n",
    "def compute_auc(pos_score, neg_score):\n",
    "    scores = th.cat([pos_score, neg_score]).numpy()\n",
    "    labels = th.cat(\n",
    "        [th.ones(pos_score.shape[0]), th.zeros(neg_score.shape[0])]).numpy()\n",
    "    return roc_auc_score(labels, scores)\n",
    "\n",
    "optimizer = th.optim.Adam(itertools.chain(model.parameters(), pred.parameters()), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoque 0, loss: 0.6929873824119568\n",
      "Epoque 5, loss: 0.6595125794410706\n",
      "Epoque 10, loss: 0.5768710374832153\n",
      "Epoque 15, loss: 0.5419908165931702\n",
      "Epoque 20, loss: 0.5046783089637756\n",
      "Epoque 25, loss: 0.48118430376052856\n",
      "Epoque 30, loss: 0.45551249384880066\n",
      "Epoque 35, loss: 0.4335471987724304\n",
      "Epoque 40, loss: 0.4114689826965332\n",
      "Epoque 45, loss: 0.38669711351394653\n",
      "Epoque 50, loss: 0.36170870065689087\n",
      "Epoque 55, loss: 0.3375073969364166\n",
      "Epoque 60, loss: 0.31328362226486206\n",
      "Epoque 65, loss: 0.2881085276603699\n",
      "Epoque 70, loss: 0.2633311450481415\n",
      "Epoque 75, loss: 0.23842160403728485\n",
      "Epoque 80, loss: 0.21374496817588806\n",
      "Epoque 85, loss: 0.1896471530199051\n",
      "Epoque 90, loss: 0.1660909354686737\n",
      "Epoque 95, loss: 0.1437922567129135\n",
      "AUC 0.8452577810260975\n"
     ]
    }
   ],
   "source": [
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    h = model(train_g3, train_g3.ndata['feat'])\n",
    "    pos_score = pred(train_pos_g3, h)\n",
    "    neg_score = pred(train_neg_g3, h)\n",
    "    loss = compute_loss(pos_score, neg_score)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        print('Epoque {}, loss: {}'.format(e, loss))\n",
    "\n",
    "# On affiche les résultats\n",
    "from sklearn.metrics import roc_auc_score\n",
    "with th.no_grad():\n",
    "    pos_score = pred(test_pos_g3, h)\n",
    "    neg_score = pred(test_neg_g3, h)\n",
    "    print('AUC', compute_auc(pos_score, neg_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
