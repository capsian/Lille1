{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLNLP Project\n",
    "--------------------------------------------\n",
    "\n",
    "# Authors\n",
    "    - Selim Lakhdar\n",
    "        - selim.lakhdar.etu@univ-lille.fr\n",
    "    - Josue Happe\n",
    "        - josue.happe.etu@univ-lille.fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from transformers import AdamW\n",
    "\n",
    "random_state = 59\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>score</th>\n",
       "      <th>Right_Name</th>\n",
       "      <th>Right_Gender</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Include?</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38.0</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A'isha</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A'ishah</td>\n",
       "      <td>F</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121555</th>\n",
       "      <td>Zyvon</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zyvon</td>\n",
       "      <td>M</td>\n",
       "      <td>6.0</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121556</th>\n",
       "      <td>Zyyanna</td>\n",
       "      <td>F</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zyyanna</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121557</th>\n",
       "      <td>Zyyon</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zyyon</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121558</th>\n",
       "      <td>Zzyzx</td>\n",
       "      <td>M</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Zzyzx</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121559</th>\n",
       "      <td>undefined</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121560 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             name gender  score Right_Name Right_Gender  Frequency Include?  \\\n",
       "0               1      F    NaN        NaN          NaN       38.0       No   \n",
       "1               1      M    NaN        NaN          NaN       56.0      Yes   \n",
       "2               A      M    1.0        NaN          NaN        NaN      NaN   \n",
       "3          A'isha      F    0.0        NaN          NaN        NaN      NaN   \n",
       "4         A'ishah      F    0.0        NaN          NaN        NaN      NaN   \n",
       "...           ...    ...    ...        ...          ...        ...      ...   \n",
       "121555      Zyvon      M    1.0      Zyvon            M        6.0       No   \n",
       "121556    Zyyanna      F    1.0    Zyyanna            F        NaN      NaN   \n",
       "121557      Zyyon      M    1.0      Zyyon            M        NaN      NaN   \n",
       "121558      Zzyzx      M    1.0      Zzyzx            M        NaN      NaN   \n",
       "121559  undefined      F    NaN        NaN          NaN        NaN      NaN   \n",
       "\n",
       "        probability  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "...             ...  \n",
       "121555          1.0  \n",
       "121556          1.0  \n",
       "121557          1.0  \n",
       "121558          1.0  \n",
       "121559          NaN  \n",
       "\n",
       "[121560 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"Ultimate_Gender_List.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['length'] = data['name'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "values = data[['name', 'gender', 'length']].values\n",
    "data_clean = pd.DataFrame(values, columns=['name', 'gender', 'length'])\n",
    "data_clean = data_clean.astype({'name':'str', 'gender':'str', 'length':'int'}).dropna()\n",
    "data_clean.gender.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A'isha</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A'ishah</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A-jay</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A.j.</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aa'isha</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121554</th>\n",
       "      <td>Zyvion</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121555</th>\n",
       "      <td>Zyvon</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121556</th>\n",
       "      <td>Zyyanna</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121557</th>\n",
       "      <td>Zyyon</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121558</th>\n",
       "      <td>Zzyzx</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121556 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  gender  length\n",
       "3        A'isha       0       6\n",
       "4       A'ishah       0       7\n",
       "5         A-jay       1       5\n",
       "6          A.j.       1       4\n",
       "7       Aa'isha       0       7\n",
       "...         ...     ...     ...\n",
       "121554   Zyvion       1       6\n",
       "121555    Zyvon       1       5\n",
       "121556  Zyyanna       0       7\n",
       "121557    Zyyon       1       5\n",
       "121558    Zzyzx       1       5\n",
       "\n",
       "[121556 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove first 3 and last one\n",
    "data_clean = data_clean[3:-1]\n",
    "\n",
    "# Catgories\n",
    "data_clean['gender'] = data_clean['gender'].replace({'F':0, 'M':1, 'Unisex':2})\n",
    "\n",
    "# add name len\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A'isha</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A'ishah</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Aa'isha</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aa'ishah</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Aabha</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120770</th>\n",
       "      <td>Zhuo</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120779</th>\n",
       "      <td>Zhyrgal</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120968</th>\n",
       "      <td>Ziv</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121079</th>\n",
       "      <td>Zohar</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121156</th>\n",
       "      <td>Zorion</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6021 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            name  gender  length\n",
       "3         A'isha       0       6\n",
       "4        A'ishah       0       7\n",
       "7        Aa'isha       0       7\n",
       "8       Aa'ishah       0       8\n",
       "10         Aabha       0       5\n",
       "...          ...     ...     ...\n",
       "120770      Zhuo       2       4\n",
       "120779   Zhyrgal       2       7\n",
       "120968       Ziv       2       3\n",
       "121079     Zohar       2       5\n",
       "121156    Zorion       2       6\n",
       "\n",
       "[6021 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_female = data_clean['gender'] == 0\n",
    "mask_male = data_clean['gender'] == 1\n",
    "mask_unisex = data_clean['gender'] == 2\n",
    "\n",
    "data_female = data_clean[mask_female][:3000]\n",
    "data_male = data_clean[mask_male][:2000]\n",
    "data_unisex = data_clean[mask_unisex]\n",
    "\n",
    "data_clean = pd.concat([data_female, data_male, data_unisex])\n",
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUrklEQVR4nO3df6yk1X3f8fcnQCjlml8l2RKWBqpupPKjJt4rSuO6urdGZeM6WizV0lrI4AZpU0QlW7UqwH/ErqKViFSSCgi0m2IBgvhqVdtd5LBpCGVlpYaQXYS9LJh6a1Z0vWhXMRi4LqKCfPvHPNTjZfbembl35rI+75d0NTPnOWee7/Po8NmZM88MqSokSW34ubUuQJI0PYa+JDXE0Jekhhj6ktQQQ1+SGnLyWhewnHPPPbcuvPDCscb++Mc/5vTTT1/dglaBdY3GukZjXaP5Wa1r7969f1VVv/CeDVX1vv7buHFjjevxxx8fe+wkWddorGs01jWan9W6gD01IFNd3pGkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNWTb0k/yNJE8l+XaS/Un+Xdd+TpJHk3yvuz27b8ytSQ4keSHJ1X3tG5Ps67bdkSSTOSxJ0iDDvNJ/C/inVfVB4HJgU5IrgVuAx6pqA/BY95gkFwNbgEuATcDdSU7qnuseYCuwofvbtHqHIklazrKh313nv9g9PKX7K2AzcH/Xfj9wTXd/M7BQVW9V1YvAAeCKJOcBZ1TVE90XBx7oGyNJmoLUEP8Tle6V+l7g7wF/UFU3J/lRVZ3V1+fVqjo7yV3Ak1X1YNd+L7ALOAjcVlVXde0fAW6uqo8P2N9Weu8IWLdu3caFhYWxDu7oK69x5M2xhq7IZeefueT2xcVFZmZmplTN8KxrNNY1GusazUrrmp+f31tVs8e2D/XbO1X1DnB5krOArye5dInug9bpa4n2QfvbDmwHmJ2drbm5uWHKfI87H9rJ7fum//NCB6+dW3L77t27GfeYJsm6RmNdo7Gu0UyqrpGu3qmqHwG76a3FH+mWbOhuj3bdDgEX9A1bDxzu2tcPaJckTckwV+/8QvcKnySnAVcB3wUeBq7vul0P7OzuPwxsSXJqkovofWD7VFW9DLyR5Mruqp3r+sZIkqZgmLWP84D7u3X9nwN2VNU3kjwB7EhyA/AS8EmAqtqfZAfwHPA2cFO3PARwI3AfcBq9df5dq3kwkqSlLRv6VfUd4FcHtP8Q+OhxxmwDtg1o3wMs9XmAJGmC/EauJDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDVk2dBPckGSx5M8n2R/ks927V9K8oMkz3R/H+sbc2uSA0leSHJ1X/vGJPu6bXckyWQOS5I0yMlD9Hkb+HxVPZ3kA8DeJI92236/qv59f+ckFwNbgEuAXwL+LMmvVNU7wD3AVuBJ4BFgE7BrdQ5FkrScZV/pV9XLVfV0d/8N4Hng/CWGbAYWquqtqnoROABckeQ84IyqeqKqCngAuGalByBJGl56+Ttk5+RC4JvApcC/AT4DvA7sofdu4NUkdwFPVtWD3Zh76b2aPwjcVlVXde0fAW6uqo8P2M9Weu8IWLdu3caFhYWxDu7oK69x5M2xhq7IZeefueT2xcVFZmZmplTN8KxrNNY1GusazUrrmp+f31tVs8e2D7O8A0CSGeCrwOeq6vUk9wC/A1R3ezvwm8Cgdfpaov29jVXbge0As7OzNTc3N2yZP+XOh3Zy+76hD3HVHLx2bsntu3fvZtxjmiTrGo11jca6RjOpuoa6eifJKfQC/6Gq+hpAVR2pqneq6q+BPwSu6LofAi7oG74eONy1rx/QLkmakmGu3glwL/B8Vf1eX/t5fd0+ATzb3X8Y2JLk1CQXARuAp6rqZeCNJFd2z3kdsHOVjkOSNIRh1j4+DHwa2Jfkma7tC8CnklxOb4nmIPBbAFW1P8kO4Dl6V/7c1F25A3AjcB9wGr11fq/ckaQpWjb0q+rPGbwe/8gSY7YB2wa076H3IbAkaQ34jVxJaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNWTb0k1yQ5PEkzyfZn+SzXfs5SR5N8r3u9uy+MbcmOZDkhSRX97VvTLKv23ZHkkzmsCRJgwzzSv9t4PNV9feBK4GbklwM3AI8VlUbgMe6x3TbtgCXAJuAu5Oc1D3XPcBWYEP3t2kVj0WStIxlQ7+qXq6qp7v7bwDPA+cDm4H7u273A9d09zcDC1X1VlW9CBwArkhyHnBGVT1RVQU80DdGkjQF6eXvkJ2TC4FvApcCL1XVWX3bXq2qs5PcBTxZVQ927fcCu4CDwG1VdVXX/hHg5qr6+ID9bKX3joB169ZtXFhYGOvgjr7yGkfeHGvoilx2/plLbl9cXGRmZmZK1QzPukazVvMLlp5j79fzZV2jWWld8/Pze6tq9tj2k4d9giQzwFeBz1XV60ssxw/aUEu0v7exajuwHWB2drbm5uaGLfOn3PnQTm7fN/QhrpqD184tuX337t2Me0yTZF2jWav5BUvPsffr+bKu0UyqrqGu3klyCr3Af6iqvtY1H+mWbOhuj3bth4AL+oavBw537esHtEuSpmSYq3cC3As8X1W/17fpYeD67v71wM6+9i1JTk1yEb0PbJ+qqpeBN5Jc2T3ndX1jJElTMMx70w8Dnwb2JXmma/sCcBuwI8kNwEvAJwGqan+SHcBz9K78uamq3unG3QjcB5xGb51/1+ochiRpGMuGflX9OYPX4wE+epwx24BtA9r30PsQWJK0BvxGriQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZNnQT/LlJEeTPNvX9qUkP0jyTPf3sb5ttyY5kOSFJFf3tW9Msq/bdkeSrP7hSJKWMswr/fuATQPaf7+qLu/+HgFIcjGwBbikG3N3kpO6/vcAW4EN3d+g55QkTdCyoV9V3wReGfL5NgMLVfVWVb0IHACuSHIecEZVPVFVBTwAXDNmzZKkMaWXwct0Si4EvlFVl3aPvwR8Bngd2AN8vqpeTXIX8GRVPdj1uxfYBRwEbquqq7r2jwA3V9XHj7O/rfTeFbBu3bqNCwsLYx3c0Vde48ibYw1dkcvOP3PJ7YuLi8zMzEypmuFZ12jWan7B0nPs/Xq+rGs0K61rfn5+b1XNHtt+8pjPdw/wO0B1t7cDvwkMWqevJdoHqqrtwHaA2dnZmpubG6vIOx/aye37xj3E8R28dm7J7bt372bcY5ok6xrNWs0vWHqOvV/Pl3WNZlJ1jXX1TlUdqap3quqvgT8Erug2HQIu6Ou6Hjjcta8f0C5JmqKxQr9bo3/XJ4B3r+x5GNiS5NQkF9H7wPapqnoZeCPJld1VO9cBO1dQtyRpDMu+N03yFWAOODfJIeCLwFySy+kt0RwEfgugqvYn2QE8B7wN3FRV73RPdSO9K4FOo7fOv2sVj0OSNIRlQ7+qPjWg+d4l+m8Dtg1o3wNcOlJ1kqRV5TdyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNWTZ0E/y5SRHkzzb13ZOkkeTfK+7Pbtv261JDiR5IcnVfe0bk+zrtt2RJKt/OJKkpZw8RJ/7gLuAB/rabgEeq6rbktzSPb45ycXAFuAS4JeAP0vyK1X1DnAPsBV4EngE2ATsWq0DkaRJuPCWP16T/d636fSJPO+yr/Sr6pvAK8c0bwbu7+7fD1zT175QVW9V1YvAAeCKJOcBZ1TVE1VV9P4BuQZJ0lSll8HLdEouBL5RVZd2j39UVWf1bX+1qs5OchfwZFU92LXfS+/V/EHgtqq6qmv/CHBzVX38OPvbSu9dAevWrdu4sLAw1sEdfeU1jrw51tAVuez8M5fcvri4yMzMzJSqGZ51jWat5hcsPcfer+frRK1r3w9em2I1P3HRmSet6HzNz8/vrarZY9uHWd4ZxaB1+lqifaCq2g5sB5idna25ubmxirnzoZ3cvm+1D3F5B6+dW3L77t27GfeYJsm6RrNW8wuWnmPv1/N1otb1mTVc3pnE+Rr36p0j3ZIN3e3Rrv0QcEFfv/XA4a59/YB2SdIUjRv6DwPXd/evB3b2tW9JcmqSi4ANwFNV9TLwRpIru6t2rusbI0makmXfmyb5CjAHnJvkEPBF4DZgR5IbgJeATwJU1f4kO4DngLeBm7ordwBupHcl0Gn01vm9ckeSpmzZ0K+qTx1n00eP038bsG1A+x7g0pGqkyStKr+RK0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNWVHoJzmYZF+SZ5Ls6drOSfJoku91t2f39b81yYEkLyS5eqXFS5JGsxqv9Oer6vKqmu0e3wI8VlUbgMe6xyS5GNgCXAJsAu5OctIq7F+SNKRJLO9sBu7v7t8PXNPXvlBVb1XVi8AB4IoJ7F+SdBypqvEHJy8CrwIF/Keq2p7kR1V1Vl+fV6vq7CR3AU9W1YNd+73Arqr6LwOedyuwFWDdunUbFxYWxqrv6CuvceTNsYauyGXnn7nk9sXFRWZmZqZUzfCsazRrNb9g6Tn2fj1fJ2pd+37w2hSr+YmLzjxpRedrfn5+b98KzP938oqqgg9X1eEkvwg8muS7S/TNgLaB/+JU1XZgO8Ds7GzNzc2NVdydD+3k9n0rPcTRHbx2bsntu3fvZtxjmiTrGs1azS9Yeo69X8/XiVrXZ2754+kV0+e+TadP5HytaHmnqg53t0eBr9NbrjmS5DyA7vZo1/0QcEHf8PXA4ZXsX5I0mrFDP8npST7w7n3gnwHPAg8D13fdrgd2dvcfBrYkOTXJRcAG4Klx9y9JGt1K3puuA76e5N3n+aOq+pMkfwnsSHID8BLwSYCq2p9kB/Ac8DZwU1W9s6LqJUkjGTv0q+r7wAcHtP8Q+OhxxmwDto27T0nSyviNXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ2Zeugn2ZTkhSQHktwy7f1LUsumGvpJTgL+APh14GLgU0kunmYNktSyab/SvwI4UFXfr6r/CywAm6dcgyQ16+Qp7+984H/3PT4E/MNjOyXZCmztHi4meWHM/Z0L/NWYY8eW3122y5rUNQTrGs2a1bXMHPN8jeZ9Wdf87664rl8e1Djt0M+AtnpPQ9V2YPuKd5bsqarZlT7ParOu0VjXaKxrNK3VNe3lnUPABX2P1wOHp1yDJDVr2qH/l8CGJBcl+XlgC/DwlGuQpGZNdXmnqt5O8q+B/wacBHy5qvZPcJcrXiKaEOsajXWNxrpG01RdqXrPkrok6WeU38iVpIYY+pLUkBMy9Jf7KYf03NFt/06SDw07dsJ1XdvV850k30rywb5tB5PsS/JMkj1TrmsuyWvdvp9J8tvDjp1wXf+2r6Znk7yT5Jxu2yTP15eTHE3y7HG2r9X8Wq6utZpfy9W1VvNrubrWan5dkOTxJM8n2Z/kswP6TG6OVdUJ9UfvA+D/Bfxd4OeBbwMXH9PnY8Auet8LuBL4i2HHTriuXwPO7u7/+rt1dY8PAueu0fmaA74xzthJ1nVM/98A/vukz1f33P8E+BDw7HG2T31+DVnX1OfXkHVNfX4NU9cazq/zgA919z8A/M9pZtiJ+Ep/mJ9y2Aw8UD1PAmclOW/IsROrq6q+VVWvdg+fpPc9hUlbyTGv6fk6xqeAr6zSvpdUVd8EXlmiy1rMr2XrWqP5Ncz5Op41PV/HmOb8ermqnu7uvwE8T+/XCvpNbI6diKE/6Kccjj1hx+szzNhJ1tXvBnr/kr+rgD9Nsje9n6FYLcPW9Y+SfDvJriSXjDh2knWR5G8Cm4Cv9jVP6nwNYy3m16imNb+GNe35NbS1nF9JLgR+FfiLYzZNbI5N+2cYVsMwP+VwvD5D/QzEmIZ+7iTz9P6j/Md9zR+uqsNJfhF4NMl3u1cq06jraeCXq2oxyceA/wpsGHLsJOt6128A/6Oq+l+1Tep8DWMt5tfQpjy/hrEW82sUazK/kszQ+4fmc1X1+rGbBwxZlTl2Ir7SH+anHI7XZ5I/AzHUcyf5B8B/BjZX1Q/fba+qw93tUeDr9N7GTaWuqnq9qha7+48ApyQ5d5ixk6yrzxaOees9wfM1jLWYX0NZg/m1rDWaX6OY+vxKcgq9wH+oqr42oMvk5tgkPqiY5B+9dyffBy7iJx9kXHJMn3/OT38I8tSwYydc198BDgC/dkz76cAH+u5/C9g0xbr+Nj/5ot4VwEvduVvT89X1O5Peuuzp0zhfffu4kON/MDn1+TVkXVOfX0PWNfX5NUxdazW/umN/APgPS/SZ2Bw74ZZ36jg/5ZDkX3Xb/yPwCL1Pvw8A/wf4l0uNnWJdvw38LeDuJABvV+9X9NYBX+/aTgb+qKr+ZIp1/QvgxiRvA28CW6o3w9b6fAF8AvjTqvpx3/CJnS+AJF+hd8XJuUkOAV8ETumra+rza8i6pj6/hqxr6vNryLpgDeYX8GHg08C+JM90bV+g94/2xOeYP8MgSQ05Edf0JUljMvQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQ/4fjgFuPPtx/Q4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_clean.gender.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_clean['name'].values\n",
    "y = data_clean['gender'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "train_tokens = tokenizer.batch_encode_plus(\n",
    "    list(X_train),\n",
    "    max_length = data_clean['length'].max(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "train_seq = torch.tensor(train_tokens['input_ids']).to(device)\n",
    "train_mask = torch.tensor(train_tokens['attention_mask']).to(device)\n",
    "train_y = torch.tensor(y_train)\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "test_tokens = tokenizer.batch_encode_plus(\n",
    "    list(X_test),\n",
    "    max_length = data_clean['length'].max(),\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False\n",
    ")\n",
    "\n",
    "test_seq = torch.tensor(test_tokens['input_ids']).to(device)\n",
    "test_mask = torch.tensor(test_tokens['attention_mask']).to(device)\n",
    "test_y = torch.tensor(y_test)\n",
    "\n",
    "test_data = TensorDataset(test_seq, test_mask, test_y)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model1, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # dense layer 2\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "      \n",
    "        # dense layer 3 (Output layer)\n",
    "        self.fc3 = nn.Linear(512,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model2(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model2, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # dense layer 2\n",
    "        self.fc2 = nn.Linear(512,64)\n",
    "      \n",
    "        # dense layer 3 (Output layer)\n",
    "        self.fc3 = nn.Linear(64,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model3(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model3, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(512,64)\n",
    "        \n",
    "        # dense layer 3\n",
    "        self.fc3 = nn.Linear(64,64)\n",
    "      \n",
    "        # layer 4 (Output layer)\n",
    "        self.fc4 = nn.Linear(64,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc4(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model4(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model4, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(512,64)\n",
    "        \n",
    "        # layer 3\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        \n",
    "        # layer 4\n",
    "        self.fc4 = nn.Linear(32,32)\n",
    "      \n",
    "        # layer 5 (Output layer)\n",
    "        self.fc5 = nn.Linear(32,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 4\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc5(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model5(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model5, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # layer 2\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        \n",
    "        # layer 3 (Output layer)\n",
    "        self.fc3 = nn.Linear(512,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # second layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc3(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model6(nn.Module):\n",
    "    def __init__(self, bert):\n",
    "        super(Model6, self).__init__()\n",
    "        self.bert = bert \n",
    "      \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "      \n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # layer 1\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "        \n",
    "        # layer 2 (Output layer)\n",
    "        self.fc2 = nn.Linear(512,3)\n",
    "\n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "      \n",
    "        # first layer\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # output layer\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "      \n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optim, loss_func, epochs=5):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [r.to(device) for r in batch]\n",
    "        \n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # clear previously calculated gradients \n",
    "        model.zero_grad()        \n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "\n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = loss_func(preds, labels)\n",
    "\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "  \n",
    "\n",
    "    # returns avg loss\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_func, epochs=5):  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(test_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(test_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "            # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = loss_func(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(test_dataloader) \n",
    "\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, optim, loss_func, output_name, epochs=5):\n",
    "    # set initial loss to infinite\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    # empty lists to store training and validation loss of each epoch\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # for each epoch\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"epoch\", epoch+1, \"/\", epochs)\n",
    "\n",
    "        # train model\n",
    "        train_loss = train(model, optim, loss_func)\n",
    "\n",
    "        # evaluate model\n",
    "        test_loss = test(model, loss_func)\n",
    "\n",
    "        # save the best model\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            torch.save(model.state_dict(), output_name)\n",
    "\n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.3f}')\n",
    "        print(f'Validation Loss: {test_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [0.66320428 1.01493256 1.97284644]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/SD/lib/python3.8/site-packages/sklearn/utils/validation.py:70: FutureWarning: Pass classes=[0 1 2], y=[0 0 1 ... 0 2 1] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  warnings.warn(f\"Pass {args_msg} as keyword args. From version \"\n"
     ]
    }
   ],
   "source": [
    "model1 = Model1(bert).to(device)\n",
    "model2 = Model2(bert).to(device)\n",
    "model3 = Model3(bert).to(device)\n",
    "model4 = Model4(bert).to(device)\n",
    "model5 = Model5(bert).to(device)\n",
    "model6 = Model6(bert).to(device)\n",
    "\n",
    "# init weights\n",
    "weights = compute_class_weight('balanced', np.unique(y_train), y_train)\n",
    "print(\"weights:\", weights)\n",
    "weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "\n",
    "# loss function\n",
    "loss_function  = nn.NLLLoss(weight=weights) \n",
    "\n",
    "# define the optimizer\n",
    "optimizer1 = AdamW(model1.parameters(), lr = 1e-3)\n",
    "optimizer2 = AdamW(model2.parameters(), lr = 1e-3)\n",
    "optimizer3 = AdamW(model3.parameters(), lr = 1e-3)\n",
    "optimizer4 = AdamW(model4.parameters(), lr = 1e-3)\n",
    "optimizer5 = AdamW(model5.parameters(), lr = 1e-3)\n",
    "optimizer6 = AdamW(model6.parameters(), lr = 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.111\n",
      "Validation Loss: 1.098\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.096\n",
      "Validation Loss: 1.091\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.091\n",
      "Validation Loss: 1.060\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.065\n",
      "Validation Loss: 0.992\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.039\n",
      "Validation Loss: 0.943\n"
     ]
    }
   ],
   "source": [
    "evaluate(model1, optimizer1, loss_function, 'model1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.110\n",
      "Validation Loss: 1.098\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.103\n",
      "Validation Loss: 1.085\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.097\n",
      "Validation Loss: 1.075\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.082\n",
      "Validation Loss: 1.067\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.060\n",
      "Validation Loss: 1.021\n"
     ]
    }
   ],
   "source": [
    "evaluate(model2, optimizer2, loss_function, 'model2.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.106\n",
      "Validation Loss: 1.097\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.100\n",
      "Validation Loss: 1.091\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.095\n",
      "Validation Loss: 1.078\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.089\n",
      "Validation Loss: 1.073\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.064\n",
      "Validation Loss: 0.983\n"
     ]
    }
   ],
   "source": [
    "evaluate(model3, optimizer3, loss_function, 'model3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.101\n",
      "Validation Loss: 1.095\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.098\n",
      "Validation Loss: 1.097\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.098\n",
      "Validation Loss: 1.090\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.095\n",
      "Validation Loss: 1.088\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.081\n",
      "Validation Loss: 1.043\n"
     ]
    }
   ],
   "source": [
    "evaluate(model4, optimizer4, loss_function, 'model4.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.115\n",
      "Validation Loss: 1.090\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.103\n",
      "Validation Loss: 1.097\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.097\n",
      "Validation Loss: 1.084\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.102\n",
      "Validation Loss: 1.066\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.068\n",
      "Validation Loss: 1.015\n"
     ]
    }
   ],
   "source": [
    "evaluate(model5, optimizer5, loss_function, 'model5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.119\n",
      "Validation Loss: 1.037\n",
      "epoch 2 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.070\n",
      "Validation Loss: 1.006\n",
      "epoch 3 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.043\n",
      "Validation Loss: 1.073\n",
      "epoch 4 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.011\n",
      "Validation Loss: 0.899\n",
      "epoch 5 / 5\n",
      "  Batch    50  of    264.\n",
      "  Batch   100  of    264.\n",
      "  Batch   150  of    264.\n",
      "  Batch   200  of    264.\n",
      "  Batch   250  of    264.\n",
      "  Batch    50  of    113.\n",
      "  Batch   100  of    113.\n",
      "Training Loss: 1.003\n",
      "Validation Loss: 0.872\n"
     ]
    }
   ],
   "source": [
    "evaluate(model6, optimizer6, loss_function, 'model6.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    pred1_prob = model1(test_seq, test_mask).detach().cpu().numpy()\n",
    "    pred2_prob = model2(test_seq, test_mask).detach().cpu().numpy()\n",
    "    pred3_prob = model3(test_seq, test_mask).detach().cpu().numpy()\n",
    "    pred4_prob = model4(test_seq, test_mask).detach().cpu().numpy()\n",
    "    pred5_prob = model5(test_seq, test_mask).detach().cpu().numpy()\n",
    "    pred6_prob = model6(test_seq, test_mask).detach().cpu().numpy()\n",
    "    \n",
    "pred1 = np.argmax(pred1_prob, axis = 1)\n",
    "pred2 = np.argmax(pred2_prob, axis = 1)\n",
    "pred3 = np.argmax(pred3_prob, axis = 1)\n",
    "pred4 = np.argmax(pred4_prob, axis = 1)\n",
    "pred5 = np.argmax(pred5_prob, axis = 1)\n",
    "pred6 = np.argmax(pred6_prob, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.91      0.75       882\n",
      "           1       0.75      0.56      0.64       616\n",
      "           2       0.61      0.15      0.23       309\n",
      "\n",
      "    accuracy                           0.66      1807\n",
      "   macro avg       0.66      0.54      0.54      1807\n",
      "weighted avg       0.67      0.66      0.62      1807\n",
      "\n",
      "==================================================\n",
      "Model 2 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.83      0.69       882\n",
      "           1       0.60      0.25      0.36       616\n",
      "           2       0.35      0.34      0.34       309\n",
      "\n",
      "    accuracy                           0.55      1807\n",
      "   macro avg       0.51      0.48      0.46      1807\n",
      "weighted avg       0.55      0.55      0.52      1807\n",
      "\n",
      "==================================================\n",
      "Model 3 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.80      0.78       882\n",
      "           1       0.68      0.56      0.61       616\n",
      "           2       0.35      0.42      0.38       309\n",
      "\n",
      "    accuracy                           0.65      1807\n",
      "   macro avg       0.59      0.59      0.59      1807\n",
      "weighted avg       0.66      0.65      0.65      1807\n",
      "\n",
      "==================================================\n",
      "Model 4 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.30      0.45       882\n",
      "           1       0.39      0.97      0.56       616\n",
      "           2       0.00      0.00      0.00       309\n",
      "\n",
      "    accuracy                           0.48      1807\n",
      "   macro avg       0.44      0.42      0.34      1807\n",
      "weighted avg       0.58      0.48      0.41      1807\n",
      "\n",
      "==================================================\n",
      "Model 5 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.31      0.46       882\n",
      "           1       0.89      0.20      0.33       616\n",
      "           2       0.21      0.93      0.35       309\n",
      "\n",
      "    accuracy                           0.38      1807\n",
      "   macro avg       0.66      0.48      0.38      1807\n",
      "weighted avg       0.76      0.38      0.40      1807\n",
      "\n",
      "==================================================\n",
      "Model 6 scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.77      0.78       882\n",
      "           1       0.62      0.80      0.70       616\n",
      "           2       0.50      0.26      0.34       309\n",
      "\n",
      "    accuracy                           0.69      1807\n",
      "   macro avg       0.64      0.61      0.61      1807\n",
      "weighted avg       0.69      0.69      0.68      1807\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/SD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda/envs/SD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda/envs/SD/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(\"Model 1 scores:\")\n",
    "print(classification_report(y_test, pred1))\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 2 scores:\")\n",
    "print(classification_report(y_test, pred2))\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 3 scores:\")\n",
    "print(classification_report(y_test, pred3))\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 4 scores:\")\n",
    "print(classification_report(y_test, pred4))\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 5 scores:\")\n",
    "print(classification_report(y_test, pred5))\n",
    "print(\"=\" * 50)\n",
    "print(\"Model 6 scores:\")\n",
    "print(classification_report(y_test, pred6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
